{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"CEVAE model on IHDP\n",
    "\"\"\"\n",
    "\n",
    "import edward2 as ed\n",
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "# import tensorflow_probability as tfp\n",
    "# tfd = tf\n",
    "\n",
    "from progressbar import ETA, Bar, Percentage, ProgressBar\n",
    "\n",
    "from datasets import IHDP\n",
    "from evaluation import Evaluator\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import sem\n",
    "\n",
    "from utils import fc_net, get_y0_y1\n",
    "from argparse import ArgumentParser\n",
    "# from earlystopping import EarlyStopping\n",
    "\n",
    "class Args:\n",
    "    reps = 1 #10\n",
    "    earl = 10 #10\n",
    "    lr = 0.001 #0.001\n",
    "    opt = \"adam\"\n",
    "    epochs = 100 #100\n",
    "    print_every = 10 #10\n",
    "    true_post = True\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "args.true_post = True\n",
    "\n",
    "dataset = IHDP(replications=args.reps)\n",
    "dimx = 25\n",
    "scores = np.zeros((args.reps, 3))\n",
    "scores_test = np.zeros((args.reps, 3))\n",
    "\n",
    "M = None  # batch size during training\n",
    "d = 20  # latent dimension\n",
    "lamba = 1e-4  # weight decay\n",
    "nh, h = 3, 200  # number and size of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replication 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "epoch #0| 97%|#################################################  |ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation bound, old: -inf, new: -34.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #1|  5%|##                                                 |ETA:  0:00:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, log p(x) >= -0.499, ite_tr: 1.317, ate_tr: 0.472, pehe_tr: 0.972, rmse_f_tr: 1.045, rmse_cf_tr: 1.336, ite_te: 1.322, ate_te: 0.673, pehe_te: 1.064, dt: 5.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #10| 97%|################################################  |ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation bound, old: -34.603, new: -34.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #11|  7%|###                                               |ETA:  0:00:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100, log p(x) >= -0.472, ite_tr: 1.485, ate_tr: 0.025, pehe_tr: 1.312, rmse_f_tr: 0.927, rmse_cf_tr: 1.537, ite_te: 1.516, ate_te: 0.027, pehe_te: 1.226, dt: 3.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #21|  5%|##                                                |ETA:  0:00:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100, log p(x) >= -0.471, ite_tr: 1.462, ate_tr: 0.144, pehe_tr: 1.357, rmse_f_tr: 0.835, rmse_cf_tr: 1.517, ite_te: 1.495, ate_te: 0.223, pehe_te: 1.215, dt: 3.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #31|  0%|                                                 |ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/100, log p(x) >= -0.471, ite_tr: 1.460, ate_tr: 0.089, pehe_tr: 1.421, rmse_f_tr: 0.758, rmse_cf_tr: 1.512, ite_te: 1.431, ate_te: 0.107, pehe_te: 1.193, dt: 6.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #41|  5%|##                                                |ETA:  0:00:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/100, log p(x) >= -0.471, ite_tr: 1.429, ate_tr: 0.071, pehe_tr: 1.423, rmse_f_tr: 0.751, rmse_cf_tr: 1.482, ite_te: 1.402, ate_te: 0.119, pehe_te: 1.217, dt: 3.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #51|  5%|##                                                |ETA:  0:00:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/100, log p(x) >= -0.471, ite_tr: 1.418, ate_tr: 0.019, pehe_tr: 1.410, rmse_f_tr: 0.763, rmse_cf_tr: 1.474, ite_te: 1.375, ate_te: 0.050, pehe_te: 1.191, dt: 4.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #60| 97%|################################################  |ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation bound, old: -34.593, new: -34.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #61|  5%|##                                                |ETA:  0:00:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/100, log p(x) >= -0.471, ite_tr: 1.389, ate_tr: 0.003, pehe_tr: 1.358, rmse_f_tr: 0.744, rmse_cf_tr: 1.450, ite_te: 1.357, ate_te: 0.035, pehe_te: 1.120, dt: 4.376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #71|  5%|##                                                |ETA:  0:00:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/100, log p(x) >= -0.471, ite_tr: 1.406, ate_tr: 0.060, pehe_tr: 1.377, rmse_f_tr: 0.737, rmse_cf_tr: 1.470, ite_te: 1.398, ate_te: 0.006, pehe_te: 1.130, dt: 7.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #81|  5%|##                                                |ETA:  0:00:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100, log p(x) >= -0.471, ite_tr: 1.366, ate_tr: 0.048, pehe_tr: 1.335, rmse_f_tr: 0.758, rmse_cf_tr: 1.428, ite_te: 1.342, ate_te: 0.088, pehe_te: 1.071, dt: 5.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #91|  2%|#                                                 |ETA:  0:00:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100, log p(x) >= -0.471, ite_tr: 1.354, ate_tr: 0.049, pehe_tr: 1.307, rmse_f_tr: 0.733, rmse_cf_tr: 1.419, ite_te: 1.326, ate_te: 0.034, pehe_te: 1.060, dt: 5.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch #99| 97%|################################################  |ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/model.ckpt-4000\n",
      " Sample 100/100\n",
      " Sample 100/100\n",
      "Replication: 1/1, tr_ite: 1.346, tr_ate: 0.059, tr_pehe: 1.313, te_ite: 1.338, te_ate: 0.099, te_pehe: 1.078\n",
      "CEVAE model total scores\n",
      "train ITE: 1.346+-nan, train ATE: 0.059+-nan, train PEHE: 1.313+-nan\n",
      "test ITE: 1.338+-nan, test ATE: 0.099+-nan, test PEHE: 1.078+-nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "for i, (train, valid, test, contfeats, binfeats) in enumerate(dataset.get_train_valid_test()):\n",
    "    print('\\nReplication {}/{}'.format(i + 1, args.reps))\n",
    "    (xtr, ttr, ytr), (y_cftr, mu0tr, mu1tr) = train\n",
    "    (xva, tva, yva), (y_cfva, mu0va, mu1va) = valid\n",
    "    (xte, tte, yte), (y_cfte, mu0te, mu1te) = test\n",
    "    evaluator_test = Evaluator(yte, tte, y_cf=y_cfte, mu0=mu0te, mu1=mu1te)\n",
    "\n",
    "    # reorder features with binary first and continuous after\n",
    "    perm = binfeats + contfeats\n",
    "    xtr, xva, xte = xtr[:, perm], xva[:, perm], xte[:, perm]\n",
    "\n",
    "    xalltr, talltr, yalltr = np.concatenate([xtr, xva], axis=0), np.concatenate([ttr, tva], axis=0), np.concatenate([ytr, yva], axis=0)\n",
    "    evaluator_train = Evaluator(yalltr, talltr, y_cf=np.concatenate([y_cftr, y_cfva], axis=0),\n",
    "                                mu0=np.concatenate([mu0tr, mu0va], axis=0), mu1=np.concatenate([mu1tr, mu1va], axis=0))\n",
    "\n",
    "    # zero mean, unit variance for y during training\n",
    "    ym, ys = np.mean(ytr), np.std(ytr)\n",
    "    ytr, yva = (ytr - ym) / ys, (yva - ym) / ys\n",
    "    best_logpvalid = - np.inf\n",
    "    best_avg_loss = - np.inf\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.compat.v1.Session()\n",
    "        # sess = tf.Session()\n",
    "\n",
    "        # ed.set_seed(1)\n",
    "        initializer = tf.keras.initializers.GlorotNormal(seed = 0)\n",
    "        np.random.seed(1)\n",
    "        tf.compat.v1.set_random_seed(1)\n",
    "        \n",
    "        # x_ph_bin = tf.Variable([0,0], dtype=float, shape=[M, len(binfeats)], name='x_bin') # binary inputs\n",
    "\n",
    "        x_ph_bin = tf.compat.v1.placeholder(tf.float32, [M, len(binfeats)], name='x_bin')  # binary inputs\n",
    "        x_ph_cont = tf.compat.v1.placeholder(tf.float32, [M, len(contfeats)], name='x_cont')  # continuous inputs\n",
    "        t_ph = tf.compat.v1.placeholder(tf.float32, [M, 1])\n",
    "        y_ph = tf.compat.v1.placeholder(tf.float32, [M, 1])\n",
    "\n",
    "        x_ph = tf.concat([x_ph_bin, x_ph_cont], 1)\n",
    "        activation = tf.nn.elu\n",
    "\n",
    "        # CEVAE model (decoder)\n",
    "        # p(z)\n",
    "        z = ed.Normal(loc=tf.zeros([tf.shape(input=x_ph)[0], d]), scale=tf.ones([tf.shape(input=x_ph)[0], d]))\n",
    "\n",
    "        # p(x|z)\n",
    "        hx = fc_net(z, (nh - 1) * [h], [], 'px_z_shared', lamba=lamba, activation=activation)\n",
    "        logits = fc_net(hx, [h], [[len(binfeats), None]], 'px_z_bin'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1 = ed.Bernoulli(logits=logits, dtype=tf.float32, name='bernoulli_px_z')\n",
    "\n",
    "        mu, sigma = fc_net(hx, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2 = ed.Normal(loc=mu, scale=sigma, name='gaussian_px_z')\n",
    "\n",
    "        # p(t|z)\n",
    "        logits = fc_net(z, [h], [[1, None]], 'pt_z', lamba=lamba, activation=activation)\n",
    "        t = ed.Bernoulli(logits=logits, dtype=tf.float32)\n",
    "\n",
    "        # p(y|t,z)\n",
    "        mu2_t0 = fc_net(z, nh * [h], [[1, None]], 'py_t0z', lamba=lamba, activation=activation)\n",
    "        mu2_t1 = fc_net(z, nh * [h], [[1, None]], 'py_t1z', lamba=lamba, activation=activation)\n",
    "        y = ed.Normal(loc=t * mu2_t1 + (1. - t) * mu2_t0, scale=tf.ones_like(mu2_t0))\n",
    "\n",
    "        # CEVAE variational approximation (encoder)\n",
    "        # q(t|x)\n",
    "        logits_t = fc_net(x_ph, [d], [[1, None]], 'qt', lamba=lamba, activation=activation)\n",
    "        qt = ed.Bernoulli(logits=logits_t, dtype=tf.float32)\n",
    "        # q(y|x,t)\n",
    "        hqy = fc_net(x_ph, (nh - 1) * [h], [], 'qy_xt_shared', lamba=lamba, activation=activation)\n",
    "        mu_qy_t0 = fc_net(hqy, [h], [[1, None]], 'qy_xt0', lamba=lamba, activation=activation)\n",
    "        mu_qy_t1 = fc_net(hqy, [h], [[1, None]], 'qy_xt1', lamba=lamba, activation=activation)\n",
    "        qy = ed.Normal(loc=qt * mu_qy_t1 + (1. - qt) * mu_qy_t0, scale=tf.ones_like(mu_qy_t0))\n",
    "        # q(z|x,t,y)\n",
    "        inpt2 = tf.concat([x_ph, qy], 1)\n",
    "        hqz = fc_net(inpt2, (nh - 1) * [h], [], 'qz_xty_shared', lamba=lamba, activation=activation)\n",
    "        muq_t0, sigmaq_t0 = fc_net(hqz, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1, sigmaq_t1 = fc_net(hqz, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz = ed.Normal(loc=qt * muq_t1 + (1. - qt) * muq_t0, scale=qt * sigmaq_t1 + (1. - qt) * sigmaq_t0)\n",
    "        \n",
    "        # Create data dictionary for edward\n",
    "        data = {x1: x_ph_bin, x2: x_ph_cont, y: y_ph, qt: t_ph, t: t_ph, qy: y_ph}\n",
    "        \n",
    "        # Compute expected log-likelihood. First, sample from the variational distribution; second, compute the log-likelihood given the sample.\n",
    "        \n",
    "        # sample posterior predictive for p(y|z,t)\n",
    "        # y_post = ed.copy(y, {z: qz, t: t_ph}, scope='y_post')\n",
    "        mu2_t0_y_post = fc_net(qz, nh * [h], [[1, None]], 'py_t0z_y_post', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post = fc_net(qz, nh * [h], [[1, None]], 'py_t1z_y_post', lamba=lamba, activation=activation)\n",
    "        y_post_dist = ed.Normal(loc=t_ph * mu2_t1_y_post + (1. - t_ph) * mu2_t0_y_post, scale=tf.ones_like(mu2_t0_y_post))\n",
    "        # y_post = y_post_dist.distribution.sample(seed = 0)\n",
    "        \n",
    "        # crude approximation of the above\n",
    "        # y_post_mean = ed.copy(y, {z: qz.mean(), t: t_ph}, scope='y_post_mean')\n",
    "        mu2_t0_y_post_mean = fc_net(qz.distribution.mean(), nh * [h], [[1, None]], 'py_t0z_y_post_mean', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post_mean = fc_net(qz.distribution.mean(), nh * [h], [[1, None]], 'py_t1z_y_post_mean', lamba=lamba, activation=activation)\n",
    "        y_post_mean_dist = ed.Normal(loc=t_ph * mu2_t1_y_post_mean + (1. - t_ph) * mu2_t0_y_post_mean, scale=tf.ones_like(mu2_t0_y_post_mean))\n",
    "        y_post_mean = y_post_mean_dist.distribution.sample()\n",
    "        \n",
    "        # construct a deterministic version (i.e. use the mean of the approximate posterior) of the lower bound\n",
    "        # for early stopping according to a validation set\n",
    "        #  y_post_eval = ed.copy(y, {z: qz.mean(), qt: t_ph, qy: y_ph, t: t_ph}, scope='y_post_eval')\n",
    "        inpt2_y_post_eval = tf.concat([x_ph, y_ph], 1)\n",
    "        hqz_y_post_eval = fc_net(inpt2_y_post_eval, (nh - 1) * [h], [], 'qz_xty_shared_y_post_eval', lamba=lamba, activation=activation)\n",
    "        muq_t0_y_post_eval, sigmaq_t0_y_post_eval = fc_net(hqz_y_post_eval, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_y_post_eval', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_y_post_eval, sigmaq_t1_y_post_eval = fc_net(hqz_y_post_eval, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_y_post_eval', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_y_post_eval = ed.Normal(loc=t_ph * muq_t1_y_post_eval + (1. - t_ph) * muq_t0_y_post_eval, scale=t_ph * sigmaq_t1_y_post_eval + (1. - t_ph) * sigmaq_t0_y_post_eval)\n",
    "        mu2_t0_y_post_eval = fc_net(qz_y_post_eval.distribution.mean(), nh * [h], [[1, None]], 'py_t0z_y_post_eval', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post_eval = fc_net(qz_y_post_eval.distribution.mean(), nh * [h], [[1, None]], 'py_t1z_y_post_eval', lamba=lamba, activation=activation)\n",
    "        y_post_eval_dist = ed.Normal(loc=t_ph * mu2_t1_y_post_eval + (1. - t_ph) * mu2_t0_y_post_eval, scale=tf.ones_like(mu2_t0))\n",
    "        # y_post_eval = y_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # x1_post_eval = x1, {z: qz.mean(), qt: t_ph, qy: y_ph}\n",
    "        inpt2_x_post_eval = tf.concat([x_ph, y_ph], 1)\n",
    "        hqz_x_post_eval = fc_net(inpt2_x_post_eval, (nh - 1) * [h], [], 'qz_xty_shared_x_post_eval', lamba=lamba, activation=activation)\n",
    "        muq_t0_x_post_eval, sigmaq_t0_x_post_eval = fc_net(hqz_x_post_eval, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_x_post_eval', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_x_post_eval, sigmaq_t1_x_post_eval = fc_net(hqz_x_post_eval, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_x_post_eval', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_x_post_eval = ed.Normal(loc=t_ph * muq_t1_x_post_eval + (1. - t_ph) * muq_t0_x_post_eval, scale=t_ph * sigmaq_t1_x_post_eval + (1. - t_ph) * sigmaq_t0_x_post_eval)\n",
    "        hx_x_post_eval = fc_net(qz_x_post_eval.distribution.mean(), (nh - 1) * [h], [], 'px_z_shared_x_post_eval', lamba=lamba, activation=activation)\n",
    "        logits_post_eval = fc_net(hx_x_post_eval, [h], [[len(binfeats), None]], 'px_z_bin_x_post_eval'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_post_eval_dist = ed.Bernoulli(logits=logits_post_eval, dtype=tf.float32, name='bernoulli_px_z_post_eval')\n",
    "        # x1_post_eval = x1_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # x2_post_eval = x2, {z: qz.mean(), qt: t_ph, qy: y_ph}\n",
    "        mu_x2_post_eval, sigma_x2_post_eval = fc_net(hx_x_post_eval, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_x2_post_eval', lamba=lamba, activation=activation)\n",
    "        x2_post_eval_dist = ed.Normal(loc=mu_x2_post_eval, scale=sigma_x2_post_eval, name='gaussian_px_z_x2_post_eval')\n",
    "        # x2_post_eval = x2_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # t_post_eval = ed.copy(t, {z: qz.mean(), qt: t_ph, qy: y_ph}, scope='t_post_eval')\n",
    "        # logits_t_post_eval = fc_net(t_ph * muq_t1_t_post_eval + (1. - t_ph) * muq_t0_t_post_eval, [h], [[1, None]], 'pt_z_t_post_eval', lamba=lamba, activation=activation)\n",
    "        inpt2_t_post_eval = tf.concat([x_ph, y_ph], 1)\n",
    "        hqz_t_post_eval = fc_net(inpt2_t_post_eval, (nh - 1) * [h], [], 'qz_xty_shared_t_post_eval', lamba=lamba, activation=activation)\n",
    "        muq_t0_t_post_eval, sigmaq_t0_t_post_eval = fc_net(hqz_t_post_eval, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_t_post_eval', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_t_post_eval, sigmaq_t1_t_post_eval = fc_net(hqz_t_post_eval, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_t_post_eval', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_t_post_eval = ed.Normal(loc=t_ph * muq_t1_t_post_eval + (1. - t_ph) * muq_t0_t_post_eval, scale=t_ph * sigmaq_t1_t_post_eval + (1. - t_ph) * sigmaq_t0_t_post_eval)\n",
    "        logits_t_post_eval = fc_net(qz_t_post_eval.distribution.mean(), [h], [[1, None]], 'pt_z_t_post_eval', lamba=lamba, activation=activation)\n",
    "        t_post_eval_dist = ed.Bernoulli(logits=logits_t_post_eval, dtype=tf.float32)\n",
    "        # t_post_eval = y_post_eval_dist.distribution.sample()\n",
    "    \n",
    "        logp_valid = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=y_post_eval_dist.distribution.log_prob(y_ph) + t_post_eval_dist.distribution.log_prob(t_ph), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=x1_post_eval_dist.distribution.log_prob(x_ph_bin), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=x2_post_eval_dist.distribution.log_prob(x_ph_cont), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=z.distribution.log_prob(qz.distribution.mean()) - qz.distribution.log_prob(qz.distribution.mean()), axis=1)) # tf.reduce_sum(input_tensor=z.distribution.log_prob(qt * muq_t1 + (1. - qt) * muq_t0) - qz.distribution.log_prob(qt * muq_t1 + (1. - qt) * muq_t0), axis=1))\n",
    "      \n",
    "        # inference用の分布\n",
    "        logits_t_inf = fc_net(x_ph, [d], [[1, None]], 'qt_inf', lamba=lamba, activation=activation)\n",
    "        qt_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        hqy_inf = fc_net(x_ph, (nh - 1) * [h], [], 'qy_xt_shared_inf', lamba=lamba, activation=activation)\n",
    "        mu_qy_t0_inf = fc_net(hqy_inf, [h], [[1, None]], 'qy_xt0_inf', lamba=lamba, activation=activation)\n",
    "        mu_qy_t1_inf = fc_net(hqy_inf, [h], [[1, None]], 'qy_xt1_inf', lamba=lamba, activation=activation)\n",
    "        qy_inf = ed.Normal(loc=t_ph * mu_qy_t1_inf + (1. - t_ph) * mu_qy_t0_inf, scale=tf.ones_like(mu_qy_t0_inf))\n",
    "        inpt2_inf = tf.concat([x_ph, y_ph], 1)\n",
    "        hqz_inf = fc_net(inpt2_inf, (nh - 1) * [h], [], 'qz_xty_shared_inf', lamba=lamba, activation=activation)\n",
    "        muq_t0_inf, sigmaq_t0_inf = fc_net(hqz_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_inf, sigmaq_t1_inf = fc_net(hqz_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_inf = ed.Normal(loc=t_ph * muq_t1_inf + (1. - t_ph) * muq_t0_inf, scale=t_ph * sigmaq_t1_inf + (1. - t_ph) * sigmaq_t0_inf)\n",
    "        \n",
    "        hx_inf = fc_net(qz_inf.distribution.sample(seed=0), (nh - 1) * [h], [], 'px_z_shared_inf', lamba=lamba, activation=activation)\n",
    "        logits_inf = fc_net(hx_inf, [h], [[len(binfeats), None]], 'px_z_bin_inf'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf = ed.Bernoulli(logits=logits_inf, dtype=tf.float32, name='bernoulli_px_z_inf')\n",
    "        mu_inf, sigma_inf = fc_net(hx_inf, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf = ed.Normal(loc=mu_inf, scale=sigma_inf, name='gaussian_px_z_inf')\n",
    "        logits_t_inf = fc_net(qz_inf.distribution.sample(seed=0), [h], [[1, None]], 'pt_z_inf', lamba=lamba, activation=activation)\n",
    "        t_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        mu2_t0_inf = fc_net(qz_inf.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t0z_inf', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf = fc_net(qz_inf.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t1z_inf', lamba=lamba, activation=activation)\n",
    "        y_inf = ed.Normal(loc=t_inf * mu2_t1_inf + (1. - t_inf) * mu2_t0_inf, scale=tf.ones_like(mu2_t0_inf))\n",
    "        \n",
    "        inpt2_inf2 = tf.concat([x_ph, qy_inf], 1)\n",
    "        hqz_inf2 = fc_net(inpt2_inf2, (nh - 1) * [h], [], 'qz_xty_shared_inf2', lamba=lamba, activation=activation)\n",
    "        muq_t0_inf2, sigmaq_t0_inf2 = fc_net(hqz_inf2, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_inf2', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_inf2, sigmaq_t1_inf2 = fc_net(hqz_inf2, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_inf2', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_inf2 = ed.Normal(loc=t_ph * muq_t1_inf2 + (1. - t_ph) * muq_t0_inf2, scale=t_ph * sigmaq_t1_inf2 + (1. - t_ph) * sigmaq_t0_inf2)\n",
    "        hx_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), (nh - 1) * [h], [], 'px_z_shared_inf2', lamba=lamba, activation=activation)\n",
    "        logits_inf2 = fc_net(hx_inf2, [h], [[len(binfeats), None]], 'px_z_bin_inf2'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf2 = ed.Bernoulli(logits=logits_inf2, dtype=tf.float32, name='bernoulli_px_z_inf2')\n",
    "        mu_inf2, sigma_inf2 = fc_net(hx_inf2, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf2', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf2 = ed.Normal(loc=mu_inf2, scale=sigma_inf2, name='gaussian_px_z_inf2')\n",
    "        logits_t_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), [h], [[1, None]], 'pt_z_inf2', lamba=lamba, activation=activation)\n",
    "        t_inf2 = ed.Bernoulli(logits=logits_t_inf2, dtype=tf.float32)\n",
    "        mu2_t0_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t0z_inf2', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t1z_inf2', lamba=lamba, activation=activation)\n",
    "        y_inf2 = ed.Normal(loc=t_inf2 * mu2_t1_inf2 + (1. - t_inf2) * mu2_t0_inf2, scale=tf.ones_like(mu2_t0_inf2))\n",
    "        \n",
    "        inference = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=x1_inf.distribution.log_prob(x_ph_bin),axis=1) + tf.reduce_sum(input_tensor=x2_inf.distribution.log_prob(x_ph_cont),axis=1) + tf.reduce_sum(input_tensor=y_inf2.distribution.log_prob(y_ph),axis=1) + tf.reduce_sum(input_tensor=qt_inf.distribution.log_prob(t_ph),axis=1) + tf.reduce_sum(input_tensor=t_inf.distribution.log_prob(t_ph),axis=1) + tf.reduce_sum(input_tensor=qy_inf.distribution.log_prob(y_ph),axis=1) + tf.reduce_sum(input_tensor=z.distribution.log_prob(qz_inf.distribution.sample(seed=0))) - tf.reduce_sum(input_tensor=qz_inf.distribution.log_prob(qz_inf.distribution.sample(seed=0)),axis=1))\n",
    "        global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(args.lr).minimize(-inference,global_step=global_step)\n",
    "        \n",
    "        \"\"\"\n",
    "        inference = ed.KLqp({z: qz}, data)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.lr)\n",
    "        inference.initialize(optimizer=optimizer)\n",
    "        \"\"\"\n",
    "       \n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        # tf.compat.v1.global_variables_initializer().run(session=sess)\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        # saver = tf.compat.v1.train.Saver(slim.get_variables())\n",
    "        # kernel_initializer=initializers.glorot_uniform(seed=0)))\n",
    "\n",
    "        n_epoch, n_iter_per_epoch, idx = args.epochs, 10 * int(xtr.shape[0] / 100), np.arange(xtr.shape[0])\n",
    "\n",
    "        # dictionaries needed for evaluation\n",
    "        tr0, tr1 = np.zeros((xalltr.shape[0], 1)), np.ones((xalltr.shape[0], 1))\n",
    "        tr0t, tr1t = np.zeros((xte.shape[0], 1)), np.ones((xte.shape[0], 1))\n",
    "        f1 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr1}\n",
    "        f0 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr0}\n",
    "        f1t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr1t}\n",
    "        f0t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr0t}\n",
    "        \n",
    "        # early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "\n",
    "        # loss = np.zeros(n_epoch*n_iter_per_epoch)\n",
    "        logpvalid_list = np.zeros(n_iter_per_epoch*n_epoch)\n",
    "        \n",
    "        for epoch in range(n_epoch):\n",
    "            avg_loss_list = np.zeros(n_iter_per_epoch)\n",
    "            # avg_loss = 0.0\n",
    "            t0 = time.time()\n",
    "            widgets = [\"epoch #%d|\" % epoch, Percentage(), Bar(), ETA()]\n",
    "            pbar = ProgressBar(n_iter_per_epoch, widgets=widgets)\n",
    "            pbar.start()\n",
    "            np.random.shuffle(idx)\n",
    "            \n",
    "            for j in range(n_iter_per_epoch):\n",
    "              # info_dict = 0.0\n",
    "              pbar.update(j)\n",
    "              batch = np.random.choice(idx, 100)\n",
    "              x_train, y_train, t_train = xtr[batch], ytr[batch], ttr[batch] \n",
    "              \"\"\"\n",
    "              sess.run(train_op,feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)],\n",
    "                                                        x_ph_cont: x_train[:, len(binfeats):],\n",
    "                                                        t_ph: t_train, y_ph: y_train})\n",
    "              \"\"\"\n",
    "              _, info_dict = sess.run([train_op, inference], feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)],\n",
    "                                                        x_ph_cont: x_train[:, len(binfeats):],\n",
    "                                                        t_ph: t_train, y_ph: y_train}) \n",
    "              step = sess.run(global_step)\n",
    "              # print(info_dict)\n",
    "              logpvalid = sess.run(logp_valid, feed_dict={x_ph_bin: xva[:, 0:len(binfeats)], x_ph_cont: xva[:, len(binfeats):],\n",
    "                                                            t_ph: tva, y_ph: yva})\n",
    "              # print(logpvalid)\n",
    "              # avg_loss += info_dict['loss']\n",
    "              avg_loss_list[j] = info_dict\n",
    "              logpvalid_list[epoch*n_iter_per_epoch+j]=logpvalid\n",
    "              # avg_loss += info_dict\n",
    "              # print(avg_loss_list[j])\n",
    "              # loss[epoch*n_iter_per_epoch+j] = avg_loss_list[j]\n",
    "              # print(loss[epoch*n_iter_per_epoch+j])\n",
    "            # avg_loss = avg_loss / n_iter_per_epoch\n",
    "            avg_loss = np.mean(avg_loss_list) / n_iter_per_epoch\n",
    "            avg_loss = avg_loss / 100\n",
    "            \"\"\"\n",
    "            y0, y1 = get_y0_y1(sess, y_post_dist, f0, f1, shape=yalltr.shape, L=1)\n",
    "            y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "            score_train = evaluator_train.calc_stats(y1, y0)\n",
    "            rmses_train = evaluator_train.y_errors(y0, y1)\n",
    "            print(\"Epoch: {}/{}, avg_loss: {:0.3f}, logpvalid: {:0.3f}, ite_tr: {:0.3f}, ate_tr: {:0.3f}, pehe_tr: {:0.3f},\"\n",
    "                      \"rmse_f_tr: {:0.3f}, rmse_cf_tr: {:0.3f}, \".format(epoch + 1, n_epoch, avg_loss, logpvalid, score_train[0], score_train[1], score_train[2],\n",
    "                                           rmses_train[0], rmses_train[1]))\n",
    "            \"\"\"\n",
    "            # print(avg_loss)\n",
    "            # saver.save(sess, 'ckpt/model.ckpt', step)\n",
    "            \n",
    "            \"\"\"\n",
    "            if avg_loss >= best_avg_loss:\n",
    "                  print('Improved bound, old: {:0.3f}, new: {:0.3f}'.format(best_avg_loss, avg_loss))\n",
    "                  best_avg_loss = avg_loss\n",
    "                  saver.save(sess, \"models/m6-ihdp\")\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            if epoch % args.earl == 0 or epoch == (n_epoch - 1):\n",
    "                # logpvalid = sess.run(logp_valid, feed_dict={x_ph_bin: xva[:, 0:len(binfeats)], x_ph_cont: xva[:, len(binfeats):], t_ph: tva, y_ph: yva})\n",
    "                # print(logpvalid)\n",
    "                saver.save(sess, 'ckpt/model.ckpt', step)\n",
    "                \"\"\"\n",
    "                if early_stopping.validate(logpvalid):\n",
    "                      break\n",
    "                print('Improved validation bound, old: {:0.3f}, new: {:0.3f}'.format(best_logpvalid, logpvalid))\n",
    "                best_logpvalid = logpvalid\n",
    "                saver.save(sess, \"models/m6-ihdp\")\n",
    "                \"\"\"\n",
    "                if logpvalid >= best_logpvalid:\n",
    "                  print('Improved validation bound, old: {:0.3f}, new: {:0.3f}'.format(best_logpvalid, logpvalid))\n",
    "                  best_logpvalid = logpvalid\n",
    "                  # saver.save(sess, \"models/m6-ihdp\")\n",
    "                  # saver.save(sess, 'ckpt/model.ckpt', step)\n",
    "                  # checkpoint = tf.train.Checkpoint(sess)\n",
    "                  # manager = tf.train.CheckpointManager(checkpoint)\n",
    "                  # tf.keras.set_session(sess)\n",
    "                  # tf.keras.Model.save_weights(sess, 'models/m6-ihdp')\n",
    "            # \"\"\"\n",
    "            \n",
    "            if epoch % args.print_every == 0:\n",
    "                # y_post_train = sess.run(y_post)\n",
    "                # , feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)], x_ph_cont: x_train[:, len(binfeats):], y_ph: y_train}\n",
    "                y0, y1 = get_y0_y1(sess, y_inf2, f0, f1, shape=yalltr.shape, L=1)\n",
    "                y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "                # print(np.mean(y1-y0))\n",
    "                score_train = evaluator_train.calc_stats(y1, y0)\n",
    "                rmses_train = evaluator_train.y_errors(y0, y1)\n",
    "\n",
    "                # y_post_test = sess.run(y_post, feed_dict={x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):],  y_ph: yte})\n",
    "                y0, y1 = get_y0_y1(sess, y_inf2, f0t, f1t, shape=yte.shape, L=1)\n",
    "                y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "                score_test = evaluator_test.calc_stats(y1, y0)\n",
    "\n",
    "                print(\"Epoch: {}/{}, log p(x) >= {:0.3f}, ite_tr: {:0.3f}, ate_tr: {:0.3f}, pehe_tr: {:0.3f}, \" \\\n",
    "                      \"rmse_f_tr: {:0.3f}, rmse_cf_tr: {:0.3f}, ite_te: {:0.3f}, ate_te: {:0.3f}, pehe_te: {:0.3f}, \" \\\n",
    "                      \"dt: {:0.3f}\".format(epoch + 1, n_epoch, avg_loss, score_train[0], score_train[1], score_train[2],\n",
    "                                           rmses_train[0], rmses_train[1], score_test[0], score_test[1], score_test[2],\n",
    "                                           time.time() - t0))\n",
    "\n",
    "        ckpt_path = tf.train.latest_checkpoint('ckpt/')\n",
    "        # saver = tf.train.import_meta_graph(ckpt_path + '.meta')\n",
    "        saver.restore(sess, ckpt_path)\n",
    "        # checkpoint = tf.train.Checkpoint(sess)\n",
    "        # manager = tf.train.CheckpointManager(checkpoint)\n",
    "        # status = checkpoint.restore(manager.latest_checkpoint)\n",
    "        # tf.keras.Model.load_weights\n",
    "        # saver.restore(sess, \"models/m6-ihdp\")\n",
    "       \n",
    "        # y_post_train2 = sess.run(y_post, feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)], x_ph_cont: x_train[:, len(binfeats):], y_ph: y_train})\n",
    "        y0, y1 = get_y0_y1(sess, y_inf2, f0, f1, shape=yalltr.shape, L=100)\n",
    "        y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "        score = evaluator_train.calc_stats(y1, y0)\n",
    "        scores[i, :] = score\n",
    "\n",
    "        # y_post_test2 = sess.run(y_post, feed_dict={x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], y_ph: yte})\n",
    "        y0t, y1t = get_y0_y1(sess, y_inf2, f0t, f1t, shape=yte.shape, L=100)\n",
    "        y0t, y1t = y0t * ys + ym, y1t * ys + ym\n",
    "        score_test = evaluator_test.calc_stats(y1t, y0t)\n",
    "        scores_test[i, :] = score_test\n",
    "\n",
    "        print('Replication: {}/{}, tr_ite: {:0.3f}, tr_ate: {:0.3f}, tr_pehe: {:0.3f}' \\\n",
    "              ', te_ite: {:0.3f}, te_ate: {:0.3f}, te_pehe: {:0.3f}'.format(i + 1, args.reps,\n",
    "                                                                            score[0], score[1], score[2],\n",
    "                                                                            score_test[0], score_test[1], score_test[2]))\n",
    "        sess.close()\n",
    "\n",
    "print('CEVAE model total scores')\n",
    "means, stds = np.mean(scores, axis=0), sem(scores, axis=0)\n",
    "print('train ITE: {:.3f}+-{:.3f}, train ATE: {:.3f}+-{:.3f}, train PEHE: {:.3f}+-{:.3f}' \\\n",
    "      ''.format(means[0], stds[0], means[1], stds[1], means[2], stds[2]))\n",
    "\n",
    "means, stds = np.mean(scores_test, axis=0), sem(scores_test, axis=0)\n",
    "print('test ITE: {:.3f}+-{:.3f}, test ATE: {:.3f}+-{:.3f}, test PEHE: {:.3f}+-{:.3f}' \\\n",
    "      ''.format(means[0], stds[0], means[1], stds[1], means[2], stds[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replication 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "epoch #0| 97%|#################################################  |ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation bound, old: -inf, new: -34.756\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:304\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39;49mget_default_graph()\u001b[39m.\u001b[39;49mas_graph_element(\n\u001b[0;32m    305\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_operation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3998\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3997\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 3998\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:4086\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   4084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4085\u001b[0m   \u001b[39m# We give up!\u001b[39;00m\n\u001b[1;32m-> 4086\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not convert a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m into a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   4087\u001b[0m                   (\u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, types_str))\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a RandomVariable into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mセル3 を c:\\Users\\ayuno\\OneDrive\\画像\\デスクトップ\\研究\\卒論\\CEVAE-master\\test.ipynb\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=286'>287</a>\u001b[0m       \u001b[39m# saver.save(sess, \"models/m6-ihdp\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=287'>288</a>\u001b[0m       \u001b[39m# saver.save(sess, 'ckpt/model.ckpt', step)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=288'>289</a>\u001b[0m       \u001b[39m# checkpoint = tf.train.Checkpoint(sess)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=291'>292</a>\u001b[0m       \u001b[39m# tf.keras.Model.save_weights(sess, 'models/m6-ihdp')\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=292'>293</a>\u001b[0m \u001b[39m# \"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mprint_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m     \u001b[39m# y_post_train = sess.run(y_post)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=296'>297</a>\u001b[0m     \u001b[39m# , feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)], x_ph_cont: x_train[:, len(binfeats):], y_ph: y_train}\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=297'>298</a>\u001b[0m     y0, y1 \u001b[39m=\u001b[39m get_y0_y1(sess, qy_inf, y_inf2, f0, f1, shape\u001b[39m=\u001b[39;49myalltr\u001b[39m.\u001b[39;49mshape, L\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=298'>299</a>\u001b[0m     y0, y1 \u001b[39m=\u001b[39m y0 \u001b[39m*\u001b[39m ys \u001b[39m+\u001b[39m ym, y1 \u001b[39m*\u001b[39m ys \u001b[39m+\u001b[39m ym\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X13sZmlsZQ%3D%3D?line=299'>300</a>\u001b[0m     \u001b[39m# print(np.mean(y1-y0))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\OneDrive\\画像\\デスクトップ\\研究\\卒論\\CEVAE-master\\utils.py:50\u001b[0m, in \u001b[0;36mget_y0_y1\u001b[1;34m(sess, qy, y, f0, f1, shape, L, verbose)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_y0_y1\u001b[39m(sess, qy, y, f0, f1, shape\u001b[39m=\u001b[39m(), L\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     y0, y1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(shape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), np\u001b[39m.\u001b[39mzeros(shape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 50\u001b[0m     qy \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun(qy)\n\u001b[0;32m     51\u001b[0m     f0[\u001b[39m'\u001b[39m\u001b[39my_ph\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m qy\n\u001b[0;32m     52\u001b[0m     ymean \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mdistribution\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1176\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1173\u001b[0m       feed_map[compat\u001b[39m.\u001b[39mas_bytes(subfeed_t\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[0;32m   1175\u001b[0m \u001b[39m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[1;32m-> 1176\u001b[0m fetch_handler \u001b[39m=\u001b[39m _FetchHandler(\n\u001b[0;32m   1177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, fetches, feed_dict_tensor, feed_handles\u001b[39m=\u001b[39;49mfeed_handles)\n\u001b[0;32m   1179\u001b[0m \u001b[39m# Run request and get response.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m \u001b[39m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[39m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[39m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:485\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \n\u001b[0;32m    475\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m    direct feeds.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 485\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_mapper \u001b[39m=\u001b[39m _FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetches)\n\u001b[0;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches \u001b[39m=\u001b[39m []\n\u001b[0;32m    487\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_targets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:276\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, tensor_type):\n\u001b[0;32m    275\u001b[0m       fetches, contraction_fn \u001b[39m=\u001b[39m fetch_fn(fetch)\n\u001b[1;32m--> 276\u001b[0m       \u001b[39mreturn\u001b[39;00m _ElementFetchMapper(fetches, contraction_fn)\n\u001b[0;32m    277\u001b[0m \u001b[39m# Did not find anything.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    279\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:307\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39mas_graph_element(\n\u001b[0;32m    305\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_operation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 307\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    308\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m must be a string or Tensor. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    309\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m cannot be interpreted as \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    312\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma Tensor. (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\edward2\\tensorflow\\random_variable.py:148\u001b[0m, in \u001b[0;36mRandomVariable.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 148\u001b[0m   name \u001b[39m=\u001b[39m _numpy_text(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue)\n\u001b[0;32m    149\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mRandomVariable(\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m    150\u001b[0m       name,\n\u001b[0;32m    151\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m, shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mndims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    152\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m, dtype=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    153\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m, device=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mdevice \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\edward2\\tensorflow\\random_variable.py:201\u001b[0m, in \u001b[0;36m_numpy_text\u001b[1;34m(tensor, is_repr)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m\"\"\"Human-readable representation of a tensor's numpy value.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_numpy_compatible:\n\u001b[1;32m--> 201\u001b[0m   text \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(tensor\u001b[39m.\u001b[39mnumpy()) \u001b[39mif\u001b[39;00m is_repr \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(tensor\u001b[39m.\u001b[39;49mnumpy())\n\u001b[0;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m   text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<unprintable>\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    438\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    439\u001b[0m   \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[0;32m    440\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    445\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m--> 446\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "for i, (train, valid, test, contfeats, binfeats) in enumerate(dataset.get_train_valid_test()):\n",
    "    print('\\nReplication {}/{}'.format(i + 1, args.reps))\n",
    "    (xtr, ttr, ytr), (y_cftr, mu0tr, mu1tr) = train\n",
    "    (xva, tva, yva), (y_cfva, mu0va, mu1va) = valid\n",
    "    (xte, tte, yte), (y_cfte, mu0te, mu1te) = test\n",
    "    evaluator_test = Evaluator(yte, tte, y_cf=y_cfte, mu0=mu0te, mu1=mu1te)\n",
    "\n",
    "    # reorder features with binary first and continuous after\n",
    "    perm = binfeats + contfeats\n",
    "    xtr, xva, xte = xtr[:, perm], xva[:, perm], xte[:, perm]\n",
    "\n",
    "    xalltr, talltr, yalltr = np.concatenate([xtr, xva], axis=0), np.concatenate([ttr, tva], axis=0), np.concatenate([ytr, yva], axis=0)\n",
    "    evaluator_train = Evaluator(yalltr, talltr, y_cf=np.concatenate([y_cftr, y_cfva], axis=0),\n",
    "                                mu0=np.concatenate([mu0tr, mu0va], axis=0), mu1=np.concatenate([mu1tr, mu1va], axis=0))\n",
    "\n",
    "    # zero mean, unit variance for y during training\n",
    "    ym, ys = np.mean(ytr), np.std(ytr)\n",
    "    ytr, yva = (ytr - ym) / ys, (yva - ym) / ys\n",
    "    best_logpvalid = - np.inf\n",
    "    best_avg_loss = - np.inf\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.compat.v1.Session()\n",
    "        # sess = tf.Session()\n",
    "\n",
    "        # ed.set_seed(1)\n",
    "        initializer = tf.keras.initializers.GlorotNormal(seed = 0)\n",
    "        np.random.seed(1)\n",
    "        tf.compat.v1.set_random_seed(1)\n",
    "        \n",
    "        # x_ph_bin = tf.Variable([0,0], dtype=float, shape=[M, len(binfeats)], name='x_bin') # binary inputs\n",
    "\n",
    "        x_ph_bin = tf.compat.v1.placeholder(tf.float32, [M, len(binfeats)], name='x_bin')  # binary inputs\n",
    "        x_ph_cont = tf.compat.v1.placeholder(tf.float32, [M, len(contfeats)], name='x_cont')  # continuous inputs\n",
    "        t_ph = tf.compat.v1.placeholder(tf.float32, [M, 1])\n",
    "        y_ph = tf.compat.v1.placeholder(tf.float32, [M, 1])\n",
    "\n",
    "        x_ph = tf.concat([x_ph_bin, x_ph_cont], 1)\n",
    "        activation = tf.nn.elu\n",
    "\n",
    "        # もとの分布\n",
    "        # CEVAE model (decoder)\n",
    "        # p(z)\n",
    "        z = ed.Normal(loc=tf.zeros([tf.shape(input=x_ph)[0], d]), scale=tf.ones([tf.shape(input=x_ph)[0], d]))\n",
    "\n",
    "        # p(x|z)\n",
    "        hx = fc_net(z, (nh - 1) * [h], [], 'px_z_shared', lamba=lamba, activation=activation)\n",
    "        logits = fc_net(hx, [h], [[len(binfeats), None]], 'px_z_bin'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1 = ed.Bernoulli(logits=logits, dtype=tf.float32, name='bernoulli_px_z')\n",
    "\n",
    "        mu, sigma = fc_net(hx, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2 = ed.Normal(loc=mu, scale=sigma, name='gaussian_px_z')\n",
    "\n",
    "        # p(t|z)\n",
    "        logitst = fc_net(z, [h], [[1, None]], 'pt_z', lamba=lamba, activation=activation)\n",
    "        t = ed.Bernoulli(logits=logitst, dtype=tf.float32)\n",
    "\n",
    "        # p(y|t,z)\n",
    "        mu2_t0 = fc_net(z, nh * [h], [[1, None]], 'py_t0z', lamba=lamba, activation=activation)\n",
    "        mu2_t1 = fc_net(z, nh * [h], [[1, None]], 'py_t1z', lamba=lamba, activation=activation)\n",
    "        y = ed.Normal(loc=t * mu2_t1 + (1. - t) * mu2_t0, scale=tf.ones_like(mu2_t0))\n",
    "\n",
    "        # CEVAE variational approximation (encoder)\n",
    "        # q(t|x)\n",
    "        logits_t = fc_net(x_ph, [d], [[1, None]], 'qt', lamba=lamba, activation=activation)\n",
    "        qt = ed.Bernoulli(logits=logits_t, dtype=tf.float32)\n",
    "        # q(y|x,t)\n",
    "        hqy = fc_net(x_ph, (nh - 1) * [h], [], 'qy_xt_shared', lamba=lamba, activation=activation)\n",
    "        mu_qy_t0 = fc_net(hqy, [h], [[1, None]], 'qy_xt0', lamba=lamba, activation=activation)\n",
    "        mu_qy_t1 = fc_net(hqy, [h], [[1, None]], 'qy_xt1', lamba=lamba, activation=activation)\n",
    "        qy = ed.Normal(loc=qt * mu_qy_t1 + (1. - qt) * mu_qy_t0, scale=tf.ones_like(mu_qy_t0))\n",
    "        # q(z|x,t,y)\n",
    "        inpt2 = tf.concat([x_ph, qy], 1)\n",
    "        hqz = fc_net(inpt2, (nh - 1) * [h], [], 'qz_xty_shared', lamba=lamba, activation=activation)\n",
    "        muq_t0, sigmaq_t0 = fc_net(hqz, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1, sigmaq_t1 = fc_net(hqz, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz = ed.Normal(loc=qt * muq_t1 + (1. - qt) * muq_t0, scale=qt * sigmaq_t1 + (1. - qt) * sigmaq_t0)\n",
    "        \n",
    "        # Create data dictionary for edward\n",
    "        data = {x1: x_ph_bin, x2: x_ph_cont, y: y_ph, qt: t_ph, t: t_ph, qy: y_ph}\n",
    "        \n",
    "        # Compute expected log-likelihood. First, sample from the variational distribution; second, compute the log-likelihood given the sample.\n",
    "        \n",
    "        # sample posterior predictive for p(y|z,t)\n",
    "        # y_post = ed.copy(y, {z: qz, t: t_ph}, scope='y_post')\n",
    "        mu2_t0_y_post = fc_net(qz, nh * [h], [[1, None]], 'py_t0z_y_post', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post = fc_net(qz, nh * [h], [[1, None]], 'py_t1z_y_post', lamba=lamba, activation=activation)\n",
    "        y_post_dist = ed.Normal(loc=t_ph * mu2_t1_y_post + (1. - t_ph) * mu2_t0_y_post, scale=tf.ones_like(mu2_t0_y_post))\n",
    "        # y_post = y_post_dist.distribution.sample(seed = 0)\n",
    "        \n",
    "        # crude approximation of the above\n",
    "        # y_post_mean = ed.copy(y, {z: qz.mean(), t: t_ph}, scope='y_post_mean')\n",
    "        mu2_t0_y_post_mean = fc_net(qz.distribution.mean(), nh * [h], [[1, None]], 'py_t0z_y_post_mean', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post_mean = fc_net(qz.distribution.mean(), nh * [h], [[1, None]], 'py_t1z_y_post_mean', lamba=lamba, activation=activation)\n",
    "        y_post_mean_dist = ed.Normal(loc=t_ph * mu2_t1_y_post_mean + (1. - t_ph) * mu2_t0_y_post_mean, scale=tf.ones_like(mu2_t0_y_post_mean))\n",
    "        y_post_mean = y_post_mean_dist.distribution.sample()\n",
    "        \n",
    "        # logpvalid計算用の分布\n",
    "        # construct a deterministic version (i.e. use the mean of the approximate posterior) of the lower bound\n",
    "        # for early stopping according to a validation set\n",
    "        # qz\n",
    "        inpt2_post_eval_and_inf = tf.concat([x_ph, y_ph], 1)\n",
    "        hqz_post_eval_and_inf = fc_net(inpt2_post_eval_and_inf, (nh - 1) * [h], [], 'qz_xty_shared_post_eval_and_inf', lamba=lamba, activation=activation)\n",
    "        muq_t0_post_eval_and_inf, sigmaq_t0_post_eval_and_inf = fc_net(hqz_post_eval_and_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_post_eval_and_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_post_eval_and_inf, sigmaq_t1_post_eval_and_inf = fc_net(hqz_post_eval_and_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_post_eval_and_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_post_eval_and_inf = ed.Normal(loc=t_ph * muq_t1_post_eval_and_inf + (1. - t_ph) * muq_t0_post_eval_and_inf, scale=t_ph * sigmaq_t1_post_eval_and_inf + (1. - t_ph) * sigmaq_t0_post_eval_and_inf)\n",
    "        \n",
    "        # y_post_eval = ed.copy(y, {z: qz.mean(), qt: t_ph, qy: y_ph, t: t_ph}, scope='y_post_eval')\n",
    "        mu2_t0_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), nh * [h], [[1, None]], 'py_t0z_post_eval', lamba=lamba, activation=activation)\n",
    "        mu2_t1_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), nh * [h], [[1, None]], 'py_t1z_post_eval', lamba=lamba, activation=activation)\n",
    "        y_post_eval_dist = ed.Normal(loc=t_ph * mu2_t1_post_eval + (1. - t_ph) * mu2_t0_post_eval, scale=tf.ones_like(mu2_t0_post_eval))\n",
    "        # y_post_eval = y_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # x1_post_eval = x1, {z: qz.mean(), qt: t_ph, qy: y_ph}\n",
    "        hx_x_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), (nh - 1) * [h], [], 'px_z_shared_post_eval', lamba=lamba, activation=activation)\n",
    "        logits_post_eval = fc_net(hx_x_post_eval, [h], [[len(binfeats), None]], 'px_z_bin_x_post_eval'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_post_eval_dist = ed.Bernoulli(logits=logits_post_eval, dtype=tf.float32, name='bernoulli_px_z_post_eval')\n",
    "        # x1_post_eval = x1_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # x2_post_eval = x2, {z: qz.mean(), qt: t_ph, qy: y_ph}\n",
    "        mu_x2_post_eval, sigma_x2_post_eval = fc_net(hx_x_post_eval, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_post_eval', lamba=lamba, activation=activation)\n",
    "        x2_post_eval_dist = ed.Normal(loc=mu_x2_post_eval, scale=sigma_x2_post_eval, name='gaussian_px_z_post_eval')\n",
    "        # x2_post_eval = x2_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # t_post_eval = ed.copy(t, {z: qz.mean(), qt: t_ph, qy: y_ph}, scope='t_post_eval')\n",
    "        # logits_t_post_eval = fc_net(t_ph * muq_t1_t_post_eval + (1. - t_ph) * muq_t0_t_post_eval, [h], [[1, None]], 'pt_z_t_post_eval', lamba=lamba, activation=activation)\n",
    "        logitst_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), [h], [[1, None]], 'pt_z_post_eval', lamba=lamba, activation=activation)\n",
    "        t_post_eval_dist = ed.Bernoulli(logits=logitst_post_eval, dtype=tf.float32)\n",
    "        # t_post_eval = y_post_eval_dist.distribution.sample()\n",
    "    \n",
    "        logp_valid = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=y_post_eval_dist.distribution.log_prob(y_ph) + t_post_eval_dist.distribution.log_prob(t_ph), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=x1_post_eval_dist.distribution.log_prob(x_ph_bin), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=x2_post_eval_dist.distribution.log_prob(x_ph_cont), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=z.distribution.log_prob(qz_post_eval_and_inf.distribution.mean()) - qz_post_eval_and_inf.distribution.log_prob(qz_post_eval_and_inf.distribution.mean()), axis=1)) # tf.reduce_sum(input_tensor=z.distribution.log_prob(qt * muq_t1 + (1. - qt) * muq_t0) - qz.distribution.log_prob(qt * muq_t1 + (1. - qt) * muq_t0), axis=1))\n",
    "      \n",
    "        # inference用の分布\n",
    "        # qz_data = qz.distribution.sample()\n",
    "        \n",
    "        # 補助分布\n",
    "        logits_t_inf = fc_net(x_ph, [d], [[1, None]], 'qt_inf', lamba=lamba, activation=activation)\n",
    "        qt_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        hqy_inf = fc_net(x_ph, (nh - 1) * [h], [], 'qy_xt_shared_inf', lamba=lamba, activation=activation)\n",
    "        mu_qy_t0_inf = fc_net(hqy_inf, [h], [[1, None]], 'qy_xt0_inf', lamba=lamba, activation=activation)\n",
    "        mu_qy_t1_inf = fc_net(hqy_inf, [h], [[1, None]], 'qy_xt1_inf', lamba=lamba, activation=activation)\n",
    "        qy_inf = ed.Normal(loc=qt_inf * mu_qy_t1_inf + (1. - qt_inf) * mu_qy_t0_inf, scale=tf.ones_like(mu_qy_t0_inf))\n",
    "        \n",
    "        # 推論ネットワーク\n",
    "        hx_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), (nh - 1) * [h], [], 'px_z_shared_inf', lamba=lamba, activation=activation)\n",
    "        logits_inf = fc_net(hx_inf, [h], [[len(binfeats), None]], 'px_z_bin_inf'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf = ed.Bernoulli(logits=logits_inf, dtype=tf.float32, name='bernoulli_px_z_inf')\n",
    "        mu_inf, sigma_inf = fc_net(hx_inf, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf = ed.Normal(loc=mu_inf, scale=sigma_inf, name='gaussian_px_z_inf')\n",
    "        logits_t_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), [h], [[1, None]], 'pt_z_inf', lamba=lamba, activation=activation)\n",
    "        t_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        mu2_t0_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t0z_inf', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t1z_inf', lamba=lamba, activation=activation)\n",
    "        y_inf = ed.Normal(loc=t_inf * mu2_t1_inf + (1. - t_inf) * mu2_t0_inf, scale=tf.ones_like(mu2_t0_inf))\n",
    "        \n",
    "        # 推論ネットワーク評価指標計算用\n",
    "        inpt2_inf2 = tf.concat([x_ph, qy_inf], 1)\n",
    "        hqz_inf2 = fc_net(inpt2_inf2, (nh - 1) * [h], [], 'qz_xty_shared_inf2', lamba=lamba, activation=activation)\n",
    "        muq_t0_inf2, sigmaq_t0_inf2 = fc_net(hqz_inf2, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_inf2', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_inf2, sigmaq_t1_inf2 = fc_net(hqz_inf2, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_inf2', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_inf2 = ed.Normal(loc=t_ph * muq_t1_inf2 + (1. - t_ph) * muq_t0_inf2, scale=t_ph * sigmaq_t1_inf2 + (1. - t_ph) * sigmaq_t0_inf2)\n",
    "        hx_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), (nh - 1) * [h], [], 'px_z_shared_in2f', lamba=lamba, activation=activation)\n",
    "        logits_inf2 = fc_net(hx_inf2, [h], [[len(binfeats), None]], 'px_z_bin_inf2'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf2 = ed.Bernoulli(logits=logits_inf2, dtype=tf.float32, name='bernoulli_px_z_inf2')\n",
    "        mu_inf2, sigma_inf2 = fc_net(hx_inf2, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf2', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf2 = ed.Normal(loc=mu_inf2, scale=sigma_inf2, name='gaussian_px_z_inf2')\n",
    "        logits_t_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), [h], [[1, None]], 'pt_z_inf2', lamba=lamba, activation=activation)\n",
    "        t_inf2 = ed.Bernoulli(logits=logits_t_inf2, dtype=tf.float32)\n",
    "        mu2_t0_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t0z_inf2', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t1z_inf2', lamba=lamba, activation=activation)\n",
    "        y_inf2 = ed.Normal(loc=t_inf2 * mu2_t1_inf2 + (1. - t_inf2) * mu2_t0_inf2, scale=tf.ones_like(mu2_t0_inf2))\n",
    "        \n",
    "        inference = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=x1_inf2.distribution.log_prob(x_ph_bin),axis=1) + tf.reduce_sum(input_tensor=x2_inf2.distribution.log_prob(x_ph_cont),axis=1) + tf.reduce_sum(input_tensor=y_inf2.distribution.log_prob(y_ph),axis=1) + tf.reduce_sum(input_tensor=qt_inf.distribution.log_prob(t_ph),axis=1) + tf.reduce_sum(input_tensor=t_inf2.distribution.log_prob(t_ph),axis=1) + tf.reduce_sum(input_tensor=qy_inf.distribution.log_prob(y_ph),axis=1) + tf.reduce_sum(input_tensor=z.distribution.log_prob(qz_inf2.distribution.sample(seed=0))) - tf.reduce_sum(input_tensor=qz_inf2.distribution.log_prob(qz_inf2.distribution.sample(seed=0)),axis=1))\n",
    "        global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(args.lr).minimize(-inference,global_step=global_step)\n",
    "        \n",
    "        \"\"\"\n",
    "        inference = ed.KLqp({z: qz}, data)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.lr)\n",
    "        inference.initialize(optimizer=optimizer)\n",
    "        \"\"\"\n",
    "       \n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        # tf.compat.v1.global_variables_initializer().run(session=sess)\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        # saver = tf.compat.v1.train.Saver(slim.get_variables())\n",
    "        # kernel_initializer=initializers.glorot_uniform(seed=0)))\n",
    "\n",
    "        n_epoch, n_iter_per_epoch, idx = args.epochs, 10 * int(xtr.shape[0] / 100), np.arange(xtr.shape[0])\n",
    "\n",
    "        # dictionaries needed for evaluation\n",
    "        tr0, tr1 = np.zeros((xalltr.shape[0], 1)), np.ones((xalltr.shape[0], 1))\n",
    "        tr0t, tr1t = np.zeros((xte.shape[0], 1)), np.ones((xte.shape[0], 1))\n",
    "        f1 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr1}\n",
    "        f0 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr0}\n",
    "        f1t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr1t}\n",
    "        f0t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr0t}\n",
    "        \n",
    "        # early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "\n",
    "        # loss = np.zeros(n_epoch*n_iter_per_epoch)\n",
    "        logpvalid_list = np.zeros(n_iter_per_epoch*n_epoch)\n",
    "        \n",
    "        for epoch in range(n_epoch):\n",
    "            avg_loss_list = np.zeros(n_iter_per_epoch)\n",
    "            # avg_loss = 0.0\n",
    "            t0 = time.time()\n",
    "            widgets = [\"epoch #%d|\" % epoch, Percentage(), Bar(), ETA()]\n",
    "            pbar = ProgressBar(n_iter_per_epoch, widgets=widgets)\n",
    "            pbar.start()\n",
    "            np.random.shuffle(idx)\n",
    "            \n",
    "            for j in range(n_iter_per_epoch):\n",
    "              # info_dict = 0.0\n",
    "              pbar.update(j)\n",
    "              batch = np.random.choice(idx, 100)\n",
    "              x_train, y_train, t_train = xtr[batch], ytr[batch], ttr[batch] \n",
    "              \"\"\"\n",
    "              sess.run(train_op,feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)],\n",
    "                                                        x_ph_cont: x_train[:, len(binfeats):],\n",
    "                                                        t_ph: t_train, y_ph: y_train})\n",
    "              \"\"\"\n",
    "              _, info_dict = sess.run([train_op, inference], feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)],\n",
    "                                                        x_ph_cont: x_train[:, len(binfeats):],\n",
    "                                                        t_ph: t_train, y_ph: y_train}) \n",
    "              step = sess.run(global_step)\n",
    "              # print(info_dict)\n",
    "              logpvalid = sess.run(logp_valid, feed_dict={x_ph_bin: xva[:, 0:len(binfeats)], x_ph_cont: xva[:, len(binfeats):],\n",
    "                                                            t_ph: tva, y_ph: yva})\n",
    "              # print(logpvalid)\n",
    "              # avg_loss += info_dict['loss']\n",
    "              avg_loss_list[j] = info_dict\n",
    "              logpvalid_list[epoch*n_iter_per_epoch+j]=logpvalid\n",
    "              # avg_loss += info_dict\n",
    "              # print(avg_loss_list[j])\n",
    "              # loss[epoch*n_iter_per_epoch+j] = avg_loss_list[j]\n",
    "              # print(loss[epoch*n_iter_per_epoch+j])\n",
    "            # avg_loss = avg_loss / n_iter_per_epoch\n",
    "            avg_loss = np.mean(avg_loss_list) / n_iter_per_epoch\n",
    "            avg_loss = avg_loss / 100\n",
    "            \"\"\"\n",
    "            y0, y1 = get_y0_y1(sess, y_post_dist, f0, f1, shape=yalltr.shape, L=1)\n",
    "            y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "            score_train = evaluator_train.calc_stats(y1, y0)\n",
    "            rmses_train = evaluator_train.y_errors(y0, y1)\n",
    "            print(\"Epoch: {}/{}, avg_loss: {:0.3f}, logpvalid: {:0.3f}, ite_tr: {:0.3f}, ate_tr: {:0.3f}, pehe_tr: {:0.3f},\"\n",
    "                      \"rmse_f_tr: {:0.3f}, rmse_cf_tr: {:0.3f}, \".format(epoch + 1, n_epoch, avg_loss, logpvalid, score_train[0], score_train[1], score_train[2],\n",
    "                                           rmses_train[0], rmses_train[1]))\n",
    "            \"\"\"\n",
    "            # print(avg_loss)\n",
    "            # saver.save(sess, 'ckpt/model.ckpt', step)\n",
    "            \n",
    "            \"\"\"\n",
    "            if avg_loss >= best_avg_loss:\n",
    "                  print('Improved bound, old: {:0.3f}, new: {:0.3f}'.format(best_avg_loss, avg_loss))\n",
    "                  best_avg_loss = avg_loss\n",
    "                  saver.save(sess, \"models/m6-ihdp\")\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            if epoch % args.earl == 0 or epoch == (n_epoch - 1):\n",
    "                # logpvalid = sess.run(logp_valid, feed_dict={x_ph_bin: xva[:, 0:len(binfeats)], x_ph_cont: xva[:, len(binfeats):], t_ph: tva, y_ph: yva})\n",
    "                # print(logpvalid)\n",
    "                saver.save(sess, 'ckpt/model.ckpt', step)\n",
    "                \"\"\"\n",
    "                if early_stopping.validate(logpvalid):\n",
    "                      break\n",
    "                print('Improved validation bound, old: {:0.3f}, new: {:0.3f}'.format(best_logpvalid, logpvalid))\n",
    "                best_logpvalid = logpvalid\n",
    "                saver.save(sess, \"models/m6-ihdp\")\n",
    "                \"\"\"\n",
    "                if logpvalid >= best_logpvalid:\n",
    "                  print('Improved validation bound, old: {:0.3f}, new: {:0.3f}'.format(best_logpvalid, logpvalid))\n",
    "                  best_logpvalid = logpvalid\n",
    "                  # saver.save(sess, \"models/m6-ihdp\")\n",
    "                  # saver.save(sess, 'ckpt/model.ckpt', step)\n",
    "                  # checkpoint = tf.train.Checkpoint(sess)\n",
    "                  # manager = tf.train.CheckpointManager(checkpoint)\n",
    "                  # tf.keras.set_session(sess)\n",
    "                  # tf.keras.Model.save_weights(sess, 'models/m6-ihdp')\n",
    "            # \"\"\"\n",
    "            \n",
    "            if epoch % args.print_every == 0:\n",
    "                # y_post_train = sess.run(y_post)\n",
    "                # , feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)], x_ph_cont: x_train[:, len(binfeats):], y_ph: y_train}\n",
    "                y0, y1 = get_y0_y1(sess, qy_inf, y_inf2, f0, f1, shape=yalltr.shape, L=1)\n",
    "                y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "                # print(np.mean(y1-y0))\n",
    "                score_train = evaluator_train.calc_stats(y1, y0)\n",
    "                rmses_train = evaluator_train.y_errors(y0, y1)\n",
    "\n",
    "                # y_post_test = sess.run(y_post, feed_dict={x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):],  y_ph: yte})\n",
    "                y0, y1 = get_y0_y1(sess, qy_inf, y_inf2, f0t, f1t, shape=yte.shape, L=1)\n",
    "                y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "                score_test = evaluator_test.calc_stats(y1, y0)\n",
    "\n",
    "                print(\"Epoch: {}/{}, log p(x) >= {:0.3f}, ite_tr: {:0.3f}, ate_tr: {:0.3f}, pehe_tr: {:0.3f}, \" \\\n",
    "                      \"rmse_f_tr: {:0.3f}, rmse_cf_tr: {:0.3f}, ite_te: {:0.3f}, ate_te: {:0.3f}, pehe_te: {:0.3f}, \" \\\n",
    "                      \"dt: {:0.3f}\".format(epoch + 1, n_epoch, avg_loss, score_train[0], score_train[1], score_train[2],\n",
    "                                           rmses_train[0], rmses_train[1], score_test[0], score_test[1], score_test[2],\n",
    "                                           time.time() - t0))\n",
    "\n",
    "        ckpt_path = tf.train.latest_checkpoint('ckpt/')\n",
    "        # saver = tf.train.import_meta_graph(ckpt_path + '.meta')\n",
    "        saver.restore(sess, ckpt_path)\n",
    "        # checkpoint = tf.train.Checkpoint(sess)\n",
    "        # manager = tf.train.CheckpointManager(checkpoint)\n",
    "        # status = checkpoint.restore(manager.latest_checkpoint)\n",
    "        # tf.keras.Model.load_weights\n",
    "        # saver.restore(sess, \"models/m6-ihdp\")\n",
    "       \n",
    "        # y_post_train2 = sess.run(y_post, feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)], x_ph_cont: x_train[:, len(binfeats):], y_ph: y_train})\n",
    "        y0, y1 = get_y0_y1(sess, qy_inf, y_inf2, f0, f1, shape=yalltr.shape, L=100)\n",
    "        y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "        score = evaluator_train.calc_stats(y1, y0)\n",
    "        scores[i, :] = score\n",
    "\n",
    "        # y_post_test2 = sess.run(y_post, feed_dict={x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], y_ph: yte})\n",
    "        y0t, y1t = get_y0_y1(sess, qy_inf, y_inf2, f0t, f1t, shape=yte.shape, L=100)\n",
    "        y0t, y1t = y0t * ys + ym, y1t * ys + ym\n",
    "        score_test = evaluator_test.calc_stats(y1t, y0t)\n",
    "        scores_test[i, :] = score_test\n",
    "\n",
    "        print('Replication: {}/{}, tr_ite: {:0.3f}, tr_ate: {:0.3f}, tr_pehe: {:0.3f}' \\\n",
    "              ', te_ite: {:0.3f}, te_ate: {:0.3f}, te_pehe: {:0.3f}'.format(i + 1, args.reps,\n",
    "                                                                            score[0], score[1], score[2],\n",
    "                                                                            score_test[0], score_test[1], score_test[2]))\n",
    "        sess.close()\n",
    "\n",
    "print('CEVAE model total scores')\n",
    "means, stds = np.mean(scores, axis=0), sem(scores, axis=0)\n",
    "print('train ITE: {:.3f}+-{:.3f}, train ATE: {:.3f}+-{:.3f}, train PEHE: {:.3f}+-{:.3f}' \\\n",
    "      ''.format(means[0], stds[0], means[1], stds[1], means[2], stds[2]))\n",
    "\n",
    "means, stds = np.mean(scores_test, axis=0), sem(scores_test, axis=0)\n",
    "print('test ITE: {:.3f}+-{:.3f}, test ATE: {:.3f}+-{:.3f}, test PEHE: {:.3f}+-{:.3f}' \\\n",
    "      ''.format(means[0], stds[0], means[1], stds[1], means[2], stds[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replication 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "epoch #0| 97%|#################################################  |ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation bound, old: -inf, new: -84.743\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"Normal_9/sample_1/Reshape:0\", shape=(672, None, 1), dtype=float32) which was passed to the argument `feed_dict` with key Tensor(\"Placeholder_1:0\", shape=(None, 1), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mセル4 を c:\\Users\\ayuno\\OneDrive\\画像\\デスクトップ\\研究\\卒論\\CEVAE-master\\test.ipynb\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X15sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m       saver\u001b[39m.\u001b[39msave(sess, \u001b[39m\"\u001b[39m\u001b[39mmodels/m6-ihdp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X15sZmlsZQ%3D%3D?line=256'>257</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mprint_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X15sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m     y0, y1 \u001b[39m=\u001b[39m get_y0_y1(sess, y_inf, f0, f1, shape\u001b[39m=\u001b[39;49myalltr\u001b[39m.\u001b[39;49mshape, L\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X15sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m     y0, y1 \u001b[39m=\u001b[39m y0 \u001b[39m*\u001b[39m ys \u001b[39m+\u001b[39m ym, y1 \u001b[39m*\u001b[39m ys \u001b[39m+\u001b[39m ym\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ayuno/OneDrive/%E7%94%BB%E5%83%8F/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E7%A0%94%E7%A9%B6/%E5%8D%92%E8%AB%96/CEVAE-master/test.ipynb#X15sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m     score_train \u001b[39m=\u001b[39m evaluator_train\u001b[39m.\u001b[39mcalc_stats(y1, y0)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\OneDrive\\画像\\デスクトップ\\研究\\卒論\\CEVAE-master\\utils.py:41\u001b[0m, in \u001b[0;36mget_y0_y1\u001b[1;34m(sess, y, f0, f1, shape, L, verbose)\u001b[0m\n\u001b[0;32m     39\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m Sample \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(l \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, L))\n\u001b[0;32m     40\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m---> 41\u001b[0m     y0 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun(ymean, feed_dict\u001b[39m=\u001b[39;49mf0) \u001b[39m/\u001b[39m L\n\u001b[0;32m     42\u001b[0m     y1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39mrun(ymean, feed_dict\u001b[39m=\u001b[39mf1) \u001b[39m/\u001b[39m L\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m L \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\ayuno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1139\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1136\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot interpret feed_dict key as Tensor: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(subfeed_val, ops\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m-> 1139\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1140\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe value of a feed cannot be a tf.Tensor object. Acceptable \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1141\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mfeed values include Python scalars, strings, lists, numpy \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1142\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mndarrays, or TensorHandles. For reference, the tensor object \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1143\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwas \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(feed_val)\u001b[39m}\u001b[39;00m\u001b[39m which was passed to the argument \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1144\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`feed_dict` with key \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(feed)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1146\u001b[0m subfeed_dtype \u001b[39m=\u001b[39m subfeed_t\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mas_numpy_dtype\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(subfeed_val, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m _convert_to_numpy_obj(\n\u001b[0;32m   1148\u001b[0m     subfeed_dtype, subfeed_val) \u001b[39m!=\u001b[39m subfeed_val:\n",
      "\u001b[1;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"Normal_9/sample_1/Reshape:0\", shape=(672, None, 1), dtype=float32) which was passed to the argument `feed_dict` with key Tensor(\"Placeholder_1:0\", shape=(None, 1), dtype=float32)."
     ]
    }
   ],
   "source": [
    "for i, (train, valid, test, contfeats, binfeats) in enumerate(dataset.get_train_valid_test()):\n",
    "    print('\\nReplication {}/{}'.format(i + 1, args.reps))\n",
    "    (xtr, ttr, ytr), (y_cftr, mu0tr, mu1tr) = train\n",
    "    (xva, tva, yva), (y_cfva, mu0va, mu1va) = valid\n",
    "    (xte, tte, yte), (y_cfte, mu0te, mu1te) = test\n",
    "    evaluator_test = Evaluator(yte, tte, y_cf=y_cfte, mu0=mu0te, mu1=mu1te)\n",
    "\n",
    "    # reorder features with binary first and continuous after\n",
    "    perm = binfeats + contfeats\n",
    "    xtr, xva, xte = xtr[:, perm], xva[:, perm], xte[:, perm]\n",
    "\n",
    "    xalltr, talltr, yalltr = np.concatenate([xtr, xva], axis=0), np.concatenate([ttr, tva], axis=0), np.concatenate([ytr, yva], axis=0)\n",
    "    evaluator_train = Evaluator(yalltr, talltr, y_cf=np.concatenate([y_cftr, y_cfva], axis=0),\n",
    "                                mu0=np.concatenate([mu0tr, mu0va], axis=0), mu1=np.concatenate([mu1tr, mu1va], axis=0))\n",
    "\n",
    "    # zero mean, unit variance for y during training\n",
    "    ym, ys = np.mean(ytr), np.std(ytr)\n",
    "    ytr, yva = (ytr - ym) / ys, (yva - ym) / ys\n",
    "    best_logpvalid = - np.inf\n",
    "    best_avg_loss = - np.inf\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.compat.v1.Session()\n",
    "        # sess = tf.Session()\n",
    "\n",
    "        # ed.set_seed(1)\n",
    "        initializer = tf.keras.initializers.GlorotNormal(seed = 0)\n",
    "        np.random.seed(1)\n",
    "        tf.compat.v1.set_random_seed(1)\n",
    "        \n",
    "        # x_ph_bin = tf.Variable([0,0], dtype=float, shape=[M, len(binfeats)], name='x_bin') # binary inputs\n",
    "\n",
    "        x_ph_bin = tf.compat.v1.placeholder(tf.float32, [M, len(binfeats)], name='x_bin')  # binary inputs\n",
    "        x_ph_cont = tf.compat.v1.placeholder(tf.float32, [M, len(contfeats)], name='x_cont')  # continuous inputs\n",
    "        t_ph = tf.compat.v1.placeholder(tf.float32, [M, 1])\n",
    "        y_ph = tf.compat.v1.placeholder(tf.float32, [M, 1])\n",
    "\n",
    "        x_ph = tf.concat([x_ph_bin, x_ph_cont], 1)\n",
    "        activation = tf.nn.elu\n",
    "\n",
    "        # もとの分布\n",
    "        # CEVAE model (decoder)\n",
    "        # p(z)\n",
    "        z = ed.Normal(loc=tf.zeros([tf.shape(input=x_ph)[0], d]), scale=tf.ones([tf.shape(input=x_ph)[0], d]))\n",
    "\n",
    "        # p(x|z)\n",
    "        hx = fc_net(z, (nh - 1) * [h], [], 'px_z_shared', lamba=lamba, activation=activation)\n",
    "        logits = fc_net(hx, [h], [[len(binfeats), None]], 'px_z_bin'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1 = ed.Bernoulli(logits=logits, dtype=tf.float32, name='bernoulli_px_z')\n",
    "\n",
    "        mu, sigma = fc_net(hx, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2 = ed.Normal(loc=mu, scale=sigma, name='gaussian_px_z')\n",
    "\n",
    "        # p(t|z)\n",
    "        logitst = fc_net(z, [h], [[1, None]], 'pt_z', lamba=lamba, activation=activation)\n",
    "        t = ed.Bernoulli(logits=logitst, dtype=tf.float32)\n",
    "\n",
    "        # p(y|t,z)\n",
    "        mu2_t0 = fc_net(z, nh * [h], [[1, None]], 'py_t0z', lamba=lamba, activation=activation)\n",
    "        mu2_t1 = fc_net(z, nh * [h], [[1, None]], 'py_t1z', lamba=lamba, activation=activation)\n",
    "        y = ed.Normal(loc=t * mu2_t1 + (1. - t) * mu2_t0, scale=tf.ones_like(mu2_t0))\n",
    "\n",
    "        # CEVAE variational approximation (encoder)\n",
    "        # q(t|x)\n",
    "        logits_t = fc_net(x_ph, [d], [[1, None]], 'qt', lamba=lamba, activation=activation)\n",
    "        qt = ed.Bernoulli(logits=logits_t, dtype=tf.float32)\n",
    "        # q(y|x,t)\n",
    "        hqy = fc_net(x_ph, (nh - 1) * [h], [], 'qy_xt_shared', lamba=lamba, activation=activation)\n",
    "        mu_qy_t0 = fc_net(hqy, [h], [[1, None]], 'qy_xt0', lamba=lamba, activation=activation)\n",
    "        mu_qy_t1 = fc_net(hqy, [h], [[1, None]], 'qy_xt1', lamba=lamba, activation=activation)\n",
    "        qy = ed.Normal(loc=qt * mu_qy_t1 + (1. - qt) * mu_qy_t0, scale=tf.ones_like(mu_qy_t0))\n",
    "        # q(z|x,t,y)\n",
    "        inpt2 = tf.concat([x_ph, qy], 1)\n",
    "        hqz = fc_net(inpt2, (nh - 1) * [h], [], 'qz_xty_shared', lamba=lamba, activation=activation)\n",
    "        muq_t0, sigmaq_t0 = fc_net(hqz, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1, sigmaq_t1 = fc_net(hqz, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz = ed.Normal(loc=qt * muq_t1 + (1. - qt) * muq_t0, scale=qt * sigmaq_t1 + (1. - qt) * sigmaq_t0)\n",
    "        \n",
    "        # Create data dictionary for edward\n",
    "        data = {x1: x_ph_bin, x2: x_ph_cont, y: y_ph, qt: t_ph, t: t_ph, qy: y_ph}\n",
    "        \n",
    "        # Compute expected log-likelihood. First, sample from the variational distribution; second, compute the log-likelihood given the sample.\n",
    "        \n",
    "        # sample posterior predictive for p(y|z,t)\n",
    "        # y_post = ed.copy(y, {z: qz, t: t_ph}, scope='y_post')\n",
    "        mu2_t0_y_post = fc_net(qz, nh * [h], [[1, None]], 'py_t0z_y_post', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post = fc_net(qz, nh * [h], [[1, None]], 'py_t1z_y_post', lamba=lamba, activation=activation)\n",
    "        y_post_dist = ed.Normal(loc=t_ph * mu2_t1_y_post + (1. - t_ph) * mu2_t0_y_post, scale=tf.ones_like(mu2_t0_y_post))\n",
    "        # y_post = y_post_dist.distribution.sample(seed = 0)\n",
    "        \n",
    "        # crude approximation of the above\n",
    "        # y_post_mean = ed.copy(y, {z: qz.mean(), t: t_ph}, scope='y_post_mean')\n",
    "        mu2_t0_y_post_mean = fc_net(qz.distribution.mean(), nh * [h], [[1, None]], 'py_t0z_y_post_mean', lamba=lamba, activation=activation)\n",
    "        mu2_t1_y_post_mean = fc_net(qz.distribution.mean(), nh * [h], [[1, None]], 'py_t1z_y_post_mean', lamba=lamba, activation=activation)\n",
    "        y_post_mean_dist = ed.Normal(loc=t_ph * mu2_t1_y_post_mean + (1. - t_ph) * mu2_t0_y_post_mean, scale=tf.ones_like(mu2_t0_y_post_mean))\n",
    "        y_post_mean = y_post_mean_dist.distribution.sample()\n",
    "        \n",
    "        # logpvalid計算用の分布\n",
    "        # construct a deterministic version (i.e. use the mean of the approximate posterior) of the lower bound\n",
    "        # for early stopping according to a validation set\n",
    "        # qz\n",
    "        inpt2_post_eval_and_inf = tf.concat([x_ph, y_ph], 1)\n",
    "        hqz_post_eval_and_inf = fc_net(inpt2_post_eval_and_inf, (nh - 1) * [h], [], 'qz_xty_shared_post_eval_and_inf', lamba=lamba, activation=activation)\n",
    "        muq_t0_post_eval_and_inf, sigmaq_t0_post_eval_and_inf = fc_net(hqz_post_eval_and_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_post_eval_and_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_post_eval_and_inf, sigmaq_t1_post_eval_and_inf = fc_net(hqz_post_eval_and_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_post_eval_and_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_post_eval_and_inf = ed.Normal(loc=t_ph * muq_t1_post_eval_and_inf + (1. - t_ph) * muq_t0_post_eval_and_inf, scale=t_ph * sigmaq_t1_post_eval_and_inf + (1. - t_ph) * sigmaq_t0_post_eval_and_inf)\n",
    "        \n",
    "        # y_post_eval = ed.copy(y, {z: qz.mean(), qt: t_ph, qy: y_ph, t: t_ph}, scope='y_post_eval')\n",
    "        mu2_t0_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), nh * [h], [[1, None]], 'py_t0z_post_eval', lamba=lamba, activation=activation)\n",
    "        mu2_t1_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), nh * [h], [[1, None]], 'py_t1z_post_eval', lamba=lamba, activation=activation)\n",
    "        y_post_eval_dist = ed.Normal(loc=t_ph * mu2_t1_post_eval + (1. - t_ph) * mu2_t0_post_eval, scale=tf.ones_like(mu2_t0_post_eval))\n",
    "        # y_post_eval = y_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # x1_post_eval = x1, {z: qz.mean(), qt: t_ph, qy: y_ph}\n",
    "        hx_x_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), (nh - 1) * [h], [], 'px_z_shared_post_eval', lamba=lamba, activation=activation)\n",
    "        logits_post_eval = fc_net(hx_x_post_eval, [h], [[len(binfeats), None]], 'px_z_bin_x_post_eval'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_post_eval_dist = ed.Bernoulli(logits=logits_post_eval, dtype=tf.float32, name='bernoulli_px_z_post_eval')\n",
    "        # x1_post_eval = x1_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # x2_post_eval = x2, {z: qz.mean(), qt: t_ph, qy: y_ph}\n",
    "        mu_x2_post_eval, sigma_x2_post_eval = fc_net(hx_x_post_eval, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_post_eval', lamba=lamba, activation=activation)\n",
    "        x2_post_eval_dist = ed.Normal(loc=mu_x2_post_eval, scale=sigma_x2_post_eval, name='gaussian_px_z_post_eval')\n",
    "        # x2_post_eval = x2_post_eval_dist.distribution.sample()\n",
    "        \n",
    "        # t_post_eval = ed.copy(t, {z: qz.mean(), qt: t_ph, qy: y_ph}, scope='t_post_eval')\n",
    "        # logits_t_post_eval = fc_net(t_ph * muq_t1_t_post_eval + (1. - t_ph) * muq_t0_t_post_eval, [h], [[1, None]], 'pt_z_t_post_eval', lamba=lamba, activation=activation)\n",
    "        logitst_post_eval = fc_net(qz_post_eval_and_inf.distribution.mean(), [h], [[1, None]], 'pt_z_post_eval', lamba=lamba, activation=activation)\n",
    "        t_post_eval_dist = ed.Bernoulli(logits=logitst_post_eval, dtype=tf.float32)\n",
    "        # t_post_eval = y_post_eval_dist.distribution.sample()\n",
    "    \n",
    "        logp_valid = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=y_post_eval_dist.distribution.log_prob(y_ph) + t_post_eval_dist.distribution.log_prob(t_ph), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=x1_post_eval_dist.distribution.log_prob(x_ph_bin), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=x2_post_eval_dist.distribution.log_prob(x_ph_cont), axis=1) +\n",
    "                                    tf.reduce_sum(input_tensor=z.distribution.log_prob(qz_post_eval_and_inf.distribution.mean()) - qz_post_eval_and_inf.distribution.log_prob(qz_post_eval_and_inf.distribution.mean()), axis=1)) # tf.reduce_sum(input_tensor=z.distribution.log_prob(qt * muq_t1 + (1. - qt) * muq_t0) - qz.distribution.log_prob(qt * muq_t1 + (1. - qt) * muq_t0), axis=1))\n",
    "      \n",
    "        # inference用の分布\n",
    "        # qz_data = qz.distribution.sample()\n",
    "        \n",
    "        # 補助分布\n",
    "        logits_t_inf = fc_net(x_ph, [d], [[1, None]], 'qt_inf', lamba=lamba, activation=activation)\n",
    "        qt_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        hqy_inf = fc_net(x_ph, (nh - 1) * [h], [], 'qy_xt_shared_inf', lamba=lamba, activation=activation)\n",
    "        mu_qy_t0_inf = fc_net(hqy_inf, [h], [[1, None]], 'qy_xt0_inf', lamba=lamba, activation=activation)\n",
    "        mu_qy_t1_inf = fc_net(hqy_inf, [h], [[1, None]], 'qy_xt1_inf', lamba=lamba, activation=activation)\n",
    "        qy_inf = ed.Normal(loc=t_ph * mu_qy_t1_inf + (1. - t_ph) * mu_qy_t0_inf, scale=tf.ones_like(mu_qy_t0_inf))\n",
    "        \n",
    "        # 推論ネットワーク\n",
    "        hx_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), (nh - 1) * [h], [], 'px_z_shared_inf', lamba=lamba, activation=activation)\n",
    "        logits_inf = fc_net(hx_inf, [h], [[len(binfeats), None]], 'px_z_bin_inf'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf = ed.Bernoulli(logits=logits_inf, dtype=tf.float32, name='bernoulli_px_z_inf')\n",
    "        mu_inf, sigma_inf = fc_net(hx_inf, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf = ed.Normal(loc=mu_inf, scale=sigma_inf, name='gaussian_px_z_inf')\n",
    "        logits_t_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), [h], [[1, None]], 'pt_z_inf', lamba=lamba, activation=activation)\n",
    "        t_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        mu2_t0_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t0z_inf', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf = fc_net(qz_post_eval_and_inf.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t1z_inf', lamba=lamba, activation=activation)\n",
    "        y_inf = ed.Normal(loc=t_inf * mu2_t1_inf + (1. - t_inf) * mu2_t0_inf, scale=tf.ones_like(mu2_t0_inf))\n",
    "        \n",
    "        inference = tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=x1_inf.distribution.log_prob(x_ph_bin),axis=1) + tf.reduce_sum(input_tensor=x2_inf.distribution.log_prob(x_ph_cont),axis=1) + tf.reduce_sum(input_tensor=y_inf.distribution.log_prob(y_ph),axis=1) + tf.reduce_sum(input_tensor=qt_inf.distribution.log_prob(t_ph),axis=1) + tf.reduce_sum(input_tensor=t_inf.distribution.log_prob(t_ph),axis=1) + tf.reduce_sum(input_tensor=qy_inf.distribution.log_prob(y_ph),axis=1) + tf.reduce_sum(input_tensor=z.distribution.log_prob(qz_post_eval_and_inf.distribution.sample(seed=0))) - tf.reduce_sum(input_tensor=qz_post_eval_and_inf.distribution.log_prob(qz_post_eval_and_inf.distribution.sample(seed=0)),axis=1))\n",
    "        global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "        train_op = tf.compat.v1.train.AdamOptimizer(args.lr).minimize(-inference,global_step=global_step)\n",
    "        \n",
    "        # 推論ネットワーク評価指標計算用\n",
    "        inpt2_inf2 = tf.concat([x_ph, qy_inf], 1)\n",
    "        hqz_inf2 = fc_net(inpt2_inf2, (nh - 1) * [h], [], 'qz_xty_shared_inf2', lamba=lamba, activation=activation)\n",
    "        muq_t0_inf2, sigmaq_t0_inf2 = fc_net(hqz_inf2, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_inf2', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_inf2, sigmaq_t1_inf2 = fc_net(hqz_inf2, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_inf2', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_inf2 = ed.Normal(loc=t_ph * muq_t1_inf2 + (1. - t_ph) * muq_t0_inf2, scale=t_ph * sigmaq_t1_inf2 + (1. - t_ph) * sigmaq_t0_inf2)\n",
    "        hx_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), (nh - 1) * [h], [], 'px_z_shared_in2f', lamba=lamba, activation=activation)\n",
    "        logits_inf2 = fc_net(hx_inf2, [h], [[len(binfeats), None]], 'px_z_bin_inf2'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf2 = ed.Bernoulli(logits=logits_inf2, dtype=tf.float32, name='bernoulli_px_z_inf2')\n",
    "        mu_inf2, sigma_inf2 = fc_net(hx_inf2, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf2', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf2 = ed.Normal(loc=mu_inf2, scale=sigma_inf2, name='gaussian_px_z_inf2')\n",
    "        logits_t_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), [h], [[1, None]], 'pt_z_inf2', lamba=lamba, activation=activation)\n",
    "        t_inf2 = ed.Bernoulli(logits=logits_t_inf2, dtype=tf.float32)\n",
    "        mu2_t0_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t0z_inf2', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf2 = fc_net(qz_inf2.distribution.sample(seed=0), nh * [h], [[1, None]], 'py_t1z_inf2', lamba=lamba, activation=activation)\n",
    "        y_inf2 = ed.Normal(loc=t_inf2 * mu2_t1_inf2 + (1. - t_inf2) * mu2_t0_inf2, scale=tf.ones_like(mu2_t0_inf2))\n",
    "        \n",
    "        \"\"\"\n",
    "        inference = ed.KLqp({z: qz}, data)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.lr)\n",
    "        inference.initialize(optimizer=optimizer)\n",
    "        \"\"\"\n",
    "       \n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        # tf.compat.v1.global_variables_initializer().run(session=sess)\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        # saver = tf.compat.v1.train.Saver(slim.get_variables())\n",
    "        # kernel_initializer=initializers.glorot_uniform(seed=0)))\n",
    "\n",
    "        n_epoch, n_iter_per_epoch, idx = args.epochs, 10 * int(xtr.shape[0] / 100), np.arange(xtr.shape[0])\n",
    "\n",
    "        # dictionaries needed for evaluation\n",
    "        tr0, tr1 = np.zeros((xalltr.shape[0], 1)), np.ones((xalltr.shape[0], 1))\n",
    "        tr0t, tr1t = np.zeros((xte.shape[0], 1)), np.ones((xte.shape[0], 1))\n",
    "        qy_list = y_inf.distribution.sample(len(xalltr),seed=0)\n",
    "        \"\"\"\n",
    "        f1 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr1}\n",
    "        f0 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr0}\n",
    "        f1t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr1t}\n",
    "        f0t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr0t}\n",
    "        \"\"\"\n",
    "        f1 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr1, y_ph: qy_list}\n",
    "        f0 = {x_ph_bin: xalltr[:, 0:len(binfeats)], x_ph_cont: xalltr[:, len(binfeats):], t_ph: tr0, y_ph: qy_list}\n",
    "        f1t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr1t, y_ph: qy_list}\n",
    "        f0t = {x_ph_bin: xte[:, 0:len(binfeats)], x_ph_cont: xte[:, len(binfeats):], t_ph: tr0t, y_ph: qy_list}\n",
    "        \n",
    "        # loss = np.zeros(n_epoch*n_iter_per_epoch)\n",
    "        # logpvalid_list = np.zeros(n_iter_per_epoch*n_epoch)\n",
    "        \n",
    "        for epoch in range(n_epoch):\n",
    "            # avg_loss_list = np.zeros(n_iter_per_epoch)\n",
    "            avg_loss = 0.0\n",
    "            t0 = time.time()\n",
    "            widgets = [\"epoch #%d|\" % epoch, Percentage(), Bar(), ETA()]\n",
    "            pbar = ProgressBar(n_iter_per_epoch, widgets=widgets)\n",
    "            pbar.start()\n",
    "            np.random.shuffle(idx)\n",
    "            \n",
    "            for j in range(n_iter_per_epoch):\n",
    "              # info_dict = 0.0\n",
    "              pbar.update(j)\n",
    "              batch = np.random.choice(idx, 100)\n",
    "              x_train, y_train, t_train = xtr[batch], ytr[batch], ttr[batch] \n",
    "              \"\"\"\n",
    "              sess.run(train_op,feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)],\n",
    "                                                        x_ph_cont: x_train[:, len(binfeats):],\n",
    "                                                        t_ph: t_train, y_ph: y_train})\n",
    "              \"\"\"\n",
    "              _, info_dict = sess.run([train_op, inference], feed_dict={x_ph_bin: x_train[:, 0:len(binfeats)],\n",
    "                                                        x_ph_cont: x_train[:, len(binfeats):],\n",
    "                                                        t_ph: t_train, y_ph: y_train}) \n",
    "              step = sess.run(global_step)\n",
    "              avg_loss += info_dict\n",
    "            \n",
    "            avg_loss = avg_loss / n_iter_per_epoch\n",
    "            # avg_loss = np.mean(avg_loss_list) / n_iter_per_epoch\n",
    "            avg_loss = avg_loss / 100\n",
    "            \n",
    "            if epoch % args.earl == 0 or epoch == (n_epoch - 1):\n",
    "                logpvalid = sess.run(logp_valid, feed_dict={x_ph_bin: xva[:, 0:len(binfeats)], x_ph_cont: xva[:, len(binfeats):], t_ph: tva, y_ph: yva})\n",
    "                if logpvalid >= best_logpvalid:\n",
    "                  print('Improved validation bound, old: {:0.3f}, new: {:0.3f}'.format(best_logpvalid, logpvalid))\n",
    "                  best_logpvalid = logpvalid\n",
    "                  saver.save(sess, \"models/m6-ihdp\")\n",
    "            \n",
    "            if epoch % args.print_every == 0:\n",
    "                y0, y1 = get_y0_y1(sess, y_inf, f0, f1, shape=yalltr.shape, L=1)\n",
    "                y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "                score_train = evaluator_train.calc_stats(y1, y0)\n",
    "                rmses_train = evaluator_train.y_errors(y0, y1)\n",
    "                \n",
    "                y0, y1 = get_y0_y1(sess, y_inf, f0t, f1t, shape=yte.shape, L=1)\n",
    "                y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "                score_test = evaluator_test.calc_stats(y1, y0)\n",
    "\n",
    "                print(\"Epoch: {}/{}, log p(x) >= {:0.3f}, ite_tr: {:0.3f}, ate_tr: {:0.3f}, pehe_tr: {:0.3f}, \" \\\n",
    "                      \"rmse_f_tr: {:0.3f}, rmse_cf_tr: {:0.3f}, ite_te: {:0.3f}, ate_te: {:0.3f}, pehe_te: {:0.3f}, \" \\\n",
    "                      \"dt: {:0.3f}\".format(epoch + 1, n_epoch, avg_loss, score_train[0], score_train[1], score_train[2],\n",
    "                                           rmses_train[0], rmses_train[1], score_test[0], score_test[1], score_test[2],\n",
    "                                           time.time() - t0))\n",
    "\n",
    "        saver.restore(sess, \"models/m6-ihdp\")\n",
    "       \n",
    "        y0, y1 = get_y0_y1(sess, y_inf, f0, f1, shape=yalltr.shape, L=100)\n",
    "        y0, y1 = y0 * ys + ym, y1 * ys + ym\n",
    "        score = evaluator_train.calc_stats(y1, y0)\n",
    "        scores[i, :] = score\n",
    "\n",
    "        y0t, y1t = get_y0_y1(sess, y_inf, f0t, f1t, shape=yte.shape, L=100)\n",
    "        y0t, y1t = y0t * ys + ym, y1t * ys + ym\n",
    "        score_test = evaluator_test.calc_stats(y1t, y0t)\n",
    "        scores_test[i, :] = score_test\n",
    "\n",
    "        print('Replication: {}/{}, tr_ite: {:0.3f}, tr_ate: {:0.3f}, tr_pehe: {:0.3f}' \\\n",
    "              ', te_ite: {:0.3f}, te_ate: {:0.3f}, te_pehe: {:0.3f}'.format(i + 1, args.reps,\n",
    "                                                                            score[0], score[1], score[2],\n",
    "                                                                            score_test[0], score_test[1], score_test[2]))\n",
    "        sess.close()\n",
    "\n",
    "print('CEVAE model total scores')\n",
    "means, stds = np.mean(scores, axis=0), sem(scores, axis=0)\n",
    "print('train ITE: {:.3f}+-{:.3f}, train ATE: {:.3f}+-{:.3f}, train PEHE: {:.3f}+-{:.3f}' \\\n",
    "      ''.format(means[0], stds[0], means[1], stds[1], means[2], stds[2]))\n",
    "\n",
    "means, stds = np.mean(scores_test, axis=0), sem(scores_test, axis=0)\n",
    "print('test ITE: {:.3f}+-{:.3f}, test ATE: {:.3f}+-{:.3f}, test PEHE: {:.3f}+-{:.3f}' \\\n",
    "      ''.format(means[0], stds[0], means[1], stds[1], means[2], stds[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'edward2.tensorflow.random_variable.RandomVariable'>\n"
     ]
    }
   ],
   "source": [
    "print(type(qy_inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step= 0\n",
    "        self._loss=float('inf')\n",
    "        self._patience=patience\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def validate(self,loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self._patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self.step = 0\n",
    "            self.loss = loss\n",
    "       \n",
    "        return False\n",
    "\n",
    "\n",
    "inpt2_inf = tf.concat([x_ph, qy], 1)\n",
    "        hqz_inf = fc_net(inpt2_inf, (nh - 1) * [h], [], 'qz_xty_shared_inf', lamba=lamba, activation=activation)\n",
    "        muq_t0_inf, sigmaq_t0_inf = fc_net(hqz_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt0_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        muq_t1_inf, sigmaq_t1_inf = fc_net(hqz_inf, [h], [[d, None], [d, tf.nn.softplus]], 'qz_xt1_inf', lamba=lamba,\n",
    "                                   activation=activation)\n",
    "        qz_inf = ed.Normal(loc=qt * muq_t1_inf + (1. - qt) * muq_t0_inf, scale=qt * sigmaq_t1_inf + (1. - qt) * sigmaq_t0_inf)\n",
    "        \n",
    "        hx_inf = fc_net(qz, (nh - 1) * [h], [], 'px_z_shared_inf', lamba=lamba, activation=activation)\n",
    "        logits_inf = fc_net(hx_inf, [h], [[len(binfeats), None]], 'px_z_bin_inf'.format(i + 1), lamba=lamba, activation=activation)\n",
    "        x1_inf = ed.Bernoulli(logits=logits_inf, dtype=tf.float32, name='bernoulli_px_z_inf')\n",
    "        mu_inf, sigma_inf = fc_net(hx_inf, [h], [[len(contfeats), None], [len(contfeats), tf.nn.softplus]], 'px_z_cont_inf', lamba=lamba,\n",
    "                           activation=activation)\n",
    "        x2_inf = ed.Normal(loc=mu_inf, scale=sigma_inf, name='gaussian_px_z_inf')\n",
    "        logits_t_inf = fc_net(qz, [h], [[1, None]], 'pt_z_inf', lamba=lamba, activation=activation)\n",
    "        t_inf = ed.Bernoulli(logits=logits_t_inf, dtype=tf.float32)\n",
    "        mu2_t0_inf = fc_net(qz, nh * [h], [[1, None]], 'py_t0z_inf', lamba=lamba, activation=activation)\n",
    "        mu2_t1_inf = fc_net(qz, nh * [h], [[1, None]], 'py_t1z_inf', lamba=lamba, activation=activation)\n",
    "        y_inf = ed.Normal(loc=t_inf * mu2_t1_inf + (1. - t_inf) * mu2_t0_inf, scale=tf.ones_like(mu2_t0_inf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3593908604dd4a2f6d9d1967f677610f6fbf233a66714d4c3debee5abf9e66fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
